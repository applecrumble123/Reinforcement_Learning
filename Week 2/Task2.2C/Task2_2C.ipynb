{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task2.2C.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1bI1B1QsBKJ",
        "outputId": "e24d1e19-a135-4719-a3d4-f33d6affb9ab"
      },
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils  \n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install PyOpenGL==3.1.* \\\n",
        "            PyOpenGL-accelerate==3.1.* \\\n",
        "            gym[box2d]==0.17.* \n",
        "!pip install pyglet\n",
        "!pip install ffmpeg\n",
        "! pip install pyvirtualdisplay\n",
        "!pip install Image\n",
        "!pip install gym-maze-trustycoder83"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.7/dist-packages (1.4)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: Image in /usr/local/lib/python3.7/dist-packages (1.5.33)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.7/dist-packages (from Image) (3.1.7)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from Image) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Image) (1.15.0)\n",
            "Requirement already satisfied: sqlparse>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from django->Image) (0.4.1)\n",
            "Requirement already satisfied: asgiref<4,>=3.2.10 in /usr/local/lib/python3.7/dist-packages (from django->Image) (3.3.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from django->Image) (2018.9)\n",
            "Requirement already satisfied: gym-maze-trustycoder83 in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: pygame==1.9.6 in /usr/local/lib/python3.7/dist-packages (from gym-maze-trustycoder83) (1.9.6)\n",
            "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from gym-maze-trustycoder83) (1.18.5)\n",
            "Requirement already satisfied: gym==0.17.2 in /usr/local/lib/python3.7/dist-packages (from gym-maze-trustycoder83) (0.17.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->gym-maze-trustycoder83) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->gym-maze-trustycoder83) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->gym-maze-trustycoder83) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.2->gym-maze-trustycoder83) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzW6zNkTsLWn"
      },
      "source": [
        "If the directory vid exists and has videos left over from previous tries, its better to clean it up before continuing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVzNeQL8sSot",
        "outputId": "a5dc22ef-7cc5-4d01-c356-8642f4e2ad01"
      },
      "source": [
        "!mkdir ./vid\n",
        "!rm ./vid/*.*"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./vid’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyVGkCflscwt",
        "outputId": "09497f26-fbed-4535-9ce1-7a3f317ca7a1"
      },
      "source": [
        "import sys\n",
        "# import pygame\n",
        "import numpy as np\n",
        "# import math\n",
        "# import base64\n",
        "# import io\n",
        "# import IPython\n",
        "import gym\n",
        "import gym_maze\n",
        "\n",
        "# from gym.wrappers import Monitor\n",
        "# from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "# Recording filename\n",
        "video_name = \"./vid/Practical_2.mp4\"\n",
        "\n",
        "# Setup the environment for the maze\n",
        "env = gym.make(\"maze-sample-10x10-v0\")\n",
        "\n",
        "# Setup the video\n",
        "vid = None\n",
        "vid = video_recorder.VideoRecorder(env,video_name)\n",
        "\n",
        "# env = gym.wrappers.Monitor(env,'./vid',force=True)\n",
        "#current_state = env.reset()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_o-F2DWtlMC"
      },
      "source": [
        "import random\n",
        "\n",
        "def q_learning(discount_factor, epsilon):\n",
        "\n",
        "  current_state = env.reset()\n",
        "\n",
        "  states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "  count = 0\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "      states_dic[i, j] = count\n",
        "      count+=1\n",
        "  \n",
        "  n_actions = env.action_space.n\n",
        "  \n",
        "  # Initialize the Q-table to 0\n",
        "  q_table = np.zeros((len(states_dic),n_actions))\n",
        "\n",
        "  # Number of episode we will run\n",
        "  n_episodes = 100\n",
        "\n",
        "  # Maximum of iteration per episode\n",
        "  max_iter_episode = 500\n",
        "\n",
        "  # Initialize the exploration probability to 1\n",
        "  exploration_proba = epsilon\n",
        "\n",
        "  # discount factor\n",
        "  gamma = discount_factor\n",
        "\n",
        "\n",
        "  # Learning rate\n",
        "  lr = 1/n_actions\n",
        "\n",
        "  total_epochs, total_rewards = 0, 0\n",
        "\n",
        "  rewards_per_episode = []\n",
        "\n",
        "\n",
        "  # Iterate over episodes\n",
        "  for e in range(n_episodes):\n",
        "      \n",
        "    # We are not done yet\n",
        "    done = False\n",
        "    \n",
        "    # Sum the rewards that the agent gets from the environment\n",
        "    total_episode_epochs, total_episode_rewards = 0, 0\n",
        "    \n",
        "    for i in range(max_iter_episode):\n",
        "      \n",
        "      env.unwrapped.render()\n",
        "      vid.capture_frame()\n",
        "\n",
        "      current_coordinate_x = int(current_state[0])\n",
        "      current_coordinate_y = int(current_state[1])\n",
        "      current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "      if random.uniform(0, 1) < exploration_proba:\n",
        "        action = env.action_space.sample() # Explore action space\n",
        "      else:\n",
        "        action = int(np.argmax(q_table[current_Q_table_coordinates])) # Exploit learned values\n",
        "\n",
        "        \n",
        "      next_state, reward, done, info = env.step(action) \n",
        "\n",
        "      next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "      next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "      \n",
        "      # Update our Q-table using the Q-learning iteration\n",
        "      next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "\n",
        "      # new_value = (1 - learning_rate) * current_value + learning_rate * (reward + gamma * next_max)\n",
        "      q_table[current_Q_table_coordinates, action] = (1-lr) * q_table[current_Q_table_coordinates, action] +lr*(reward + gamma * max(q_table[next_Q_table_coordinates,:]))\n",
        "\n",
        "  \n",
        "      total_episode_rewards = total_episode_rewards + reward\n",
        "      total_episode_epochs = total_episode_epochs + 1\n",
        "        \n",
        "      if done:\n",
        "        break\n",
        "      \n",
        "      current_state = next_state\n",
        "    \n",
        "    #Reset enviroment for next episode\n",
        "    current_state = env.reset()\n",
        "        \n",
        "      \n",
        "\n",
        "    total_epochs = total_epochs + total_episode_epochs\n",
        "    total_rewards = total_rewards + total_episode_rewards\n",
        "\n",
        "\n",
        "    #Show the total episode reward        \n",
        "    #print(\"Total episode reward:\", total_episode_rewards)\n",
        "    \n",
        "    rewards_per_episode.append(total_episode_rewards)\n",
        "    \n",
        "  print(f\"Results after {n_episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {total_epochs / n_episodes}\")\n",
        "  print(f\"Average rewards per episode: {total_rewards / n_episodes}\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIhFagfQwcKW",
        "outputId": "69fb3a7f-d74c-4103-b3df-1178b9e1862c"
      },
      "source": [
        "q_learning(discount_factor = 0.5, epsilon = 0.1)\n",
        "print()\n",
        "\n",
        "q_learning(discount_factor = 0.5, epsilon = 0.3)\n",
        "print()\n",
        "\n",
        "q_learning(discount_factor = 0.5, epsilon = 0.5)\n",
        "print()\n",
        "\n",
        "q_learning(discount_factor = 0.5, epsilon = 0.7)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 86.86\n",
            "Average rewards per episode: 0.8941199999999999\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 95.28\n",
            "Average rewards per episode: 0.8656799999999997\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 112.16\n",
            "Average rewards per episode: 0.8487999999999997\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 139.66\n",
            "Average rewards per episode: 0.8012799999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DfbEPJ24P8T",
        "outputId": "dcb692f2-3c90-4d25-ddcc-b643059dfa45"
      },
      "source": [
        "q_learning(discount_factor = 0.1, epsilon = 0.1)\n",
        "print()\n",
        "\n",
        "q_learning(discount_factor = 0.3, epsilon = 0.1)\n",
        "print()\n",
        "\n",
        "q_learning(discount_factor = 0.5, epsilon = 0.1)\n",
        "print()\n",
        "\n",
        "q_learning(discount_factor = 0.7, epsilon = 0.1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 431.44\n",
            "Average rewards per episode: -0.0911000000000002\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 102.87\n",
            "Average rewards per episode: 0.8580899999999996\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 88.61\n",
            "Average rewards per episode: 0.88236\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 82.56\n",
            "Average rewards per episode: 0.8884099999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWDFe_u_6b-B"
      },
      "source": [
        "**Realistic Initialisation with discount factor = 0.7 and exploration_probability = 0.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7YpXhta6XuG",
        "outputId": "837b4117-93d7-4be3-ac3f-3047cfe9e53e"
      },
      "source": [
        "current_state = env.reset()\n",
        "\n",
        "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "count = 0\n",
        "for i in range(10):\n",
        "  for j in range(10):\n",
        "    states_dic[i, j] = count\n",
        "    count+=1\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Initialize the Q-table to 0\n",
        "q_table = np.zeros((len(states_dic),n_actions))\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 100\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "# Initialize the exploration probability to 1\n",
        "exploration_proba = 0.1\n",
        "\n",
        "# discount factor\n",
        "gamma = 0.7\n",
        "\n",
        "\n",
        "# Learning rate\n",
        "lr = 1/n_actions\n",
        "\n",
        "total_epochs, total_rewards = 0, 0\n",
        "\n",
        "rewards_per_episode = []\n",
        "\n",
        "\n",
        "# Iterate over episodes\n",
        "for e in range(n_episodes):\n",
        "    \n",
        "  # We are not done yet\n",
        "  done = False\n",
        "  \n",
        "  # Sum the rewards that the agent gets from the environment\n",
        "  total_episode_epochs, total_episode_rewards = 0, 0\n",
        "  \n",
        "  for i in range(max_iter_episode):\n",
        "    \n",
        "    env.unwrapped.render()\n",
        "    vid.capture_frame()\n",
        "\n",
        "    current_coordinate_x = int(current_state[0])\n",
        "    current_coordinate_y = int(current_state[1])\n",
        "    current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "    if random.uniform(0, 1) < exploration_proba:\n",
        "      action = env.action_space.sample() # Explore action space\n",
        "    else:\n",
        "      action = int(np.argmax(q_table[current_Q_table_coordinates])) # Exploit learned values\n",
        "\n",
        "      \n",
        "    next_state, reward, done, info = env.step(action) \n",
        "\n",
        "    next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "    next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "    \n",
        "    # Update our Q-table using the Q-learning iteration\n",
        "    next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "\n",
        "    # new_value = (1 - learning_rate) * current_value + learning_rate * (reward + gamma * next_max)\n",
        "    q_table[current_Q_table_coordinates, action] = (1-lr) * q_table[current_Q_table_coordinates, action] +lr*(reward + gamma * max(q_table[next_Q_table_coordinates,:]))\n",
        "\n",
        "\n",
        "    total_episode_rewards = total_episode_rewards + reward\n",
        "    total_episode_epochs = total_episode_epochs + 1\n",
        "      \n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "    current_state = next_state\n",
        "  \n",
        "  #Reset enviroment for next episode\n",
        "  current_state = env.reset()\n",
        "      \n",
        "  total_epochs = total_epochs + total_episode_epochs\n",
        "  total_rewards = total_rewards + total_episode_rewards\n",
        "\n",
        "  #Show the total episode reward        \n",
        "  #print(\"Total episode reward:\", total_episode_rewards)\n",
        "  \n",
        "  rewards_per_episode.append(total_episode_rewards)\n",
        "  \n",
        "print(f\"Results after {n_episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / n_episodes}\")\n",
        "print(f\"Average rewards per episode: {total_rewards / n_episodes}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 81.18\n",
            "Average rewards per episode: 0.8897899999999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTfRMVhZ6vr4"
      },
      "source": [
        "**Initialising the optimal initial value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR0XlvEX8W8s",
        "outputId": "801c0459-fb60-4fd6-84a2-0707352ec3d6"
      },
      "source": [
        "mean_each_actions = q_table.mean(axis=0)\n",
        "print(\"Mean value for each action: {}\".format(mean_each_actions), '\\n')\n",
        "\n",
        "# initialise the q-value for each action above the mean\n",
        "initialised_values = mean_each_actions + 0.05\n",
        "print(\"Initialised values for each action: {}\".format(initialised_values))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean value for each action: [0.00608487 0.00856374 0.0078227  0.02849151] \n",
            "\n",
            "Initialised values for each action: [0.05608487 0.05856374 0.0578227  0.07849151]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Decp32SI-UG"
      },
      "source": [
        "q_table_op = q_table = np.zeros((len(states_dic),n_actions))\n",
        "\n",
        "q_table_op = q_table_op + initialised_values\n",
        "\n",
        "#print(q_table_op)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_ILqCmMJQDB",
        "outputId": "ae80a1d5-2349-4365-d991-aa6123109e6e"
      },
      "source": [
        "# Number of episode we will run\n",
        "n_episodes = 100\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "# Initialize the exploration probability to 1\n",
        "exploration_proba = 0.1\n",
        "\n",
        "# discount factor\n",
        "gamma = 0.7\n",
        "\n",
        "\n",
        "# Learning rate\n",
        "lr = 1/n_actions\n",
        "\n",
        "total_epochs, total_rewards = 0, 0\n",
        "\n",
        "rewards_per_episode = []\n",
        "\n",
        "\n",
        "# Iterate over episodes\n",
        "for e in range(n_episodes):\n",
        "    \n",
        "  # We are not done yet\n",
        "  done = False\n",
        "  \n",
        "  # Sum the rewards that the agent gets from the environment\n",
        "  total_episode_epochs, total_episode_rewards = 0, 0\n",
        "  \n",
        "  for i in range(max_iter_episode):\n",
        "    \n",
        "    env.unwrapped.render()\n",
        "    vid.capture_frame()\n",
        "\n",
        "    current_coordinate_x = int(current_state[0])\n",
        "    current_coordinate_y = int(current_state[1])\n",
        "    current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "    if random.uniform(0, 1) < exploration_proba:\n",
        "      action = env.action_space.sample() # Explore action space\n",
        "    else:\n",
        "      action = int(np.argmax(q_table_op[current_Q_table_coordinates])) # Exploit learned values\n",
        "\n",
        "      \n",
        "    next_state, reward, done, info = env.step(action) \n",
        "\n",
        "    next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "    next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "    \n",
        "    # Update our Q-table using the Q-learning iteration\n",
        "    next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "\n",
        "    # new_value = (1 - learning_rate) * current_value + learning_rate * (reward + gamma * next_max)\n",
        "    q_table_op[current_Q_table_coordinates, action] = (1-lr) * q_table_op[current_Q_table_coordinates, action] +lr*(reward + gamma * max(q_table_op[next_Q_table_coordinates,:]))\n",
        "\n",
        "\n",
        "    total_episode_rewards = total_episode_rewards + reward\n",
        "    total_episode_epochs = total_episode_epochs + 1\n",
        "      \n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "    current_state = next_state\n",
        "  \n",
        "  #Reset enviroment for next episode\n",
        "  current_state = env.reset()\n",
        "      \n",
        "  total_epochs = total_epochs + total_episode_epochs\n",
        "  total_rewards = total_rewards + total_episode_rewards\n",
        "\n",
        "  #Show the total episode reward        \n",
        "  #print(\"Total episode reward:\", total_episode_rewards)\n",
        "  \n",
        "  rewards_per_episode.append(total_episode_rewards)\n",
        "  \n",
        "print(f\"Results after {n_episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / n_episodes}\")\n",
        "print(f\"Average rewards per episode: {total_rewards / n_episodes}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 78.42\n",
            "Average rewards per episode: 0.9125699999999999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}