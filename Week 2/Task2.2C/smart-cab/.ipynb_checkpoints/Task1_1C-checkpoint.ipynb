{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfS1FTLzJRNE",
    "outputId": "3018c147-1af2-4b73-a45e-74f66ce34ec4"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "def categorical_sample(prob_n, np_random):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "\n",
    "class DiscreteEnv(Env):\n",
    "\n",
    "    \"\"\"\n",
    "    Has the following members\n",
    "    - nS: number of states\n",
    "    - nA: number of actions\n",
    "    - P: transitions (*)\n",
    "    - isd: initial state distribution (**)\n",
    "    (*) dictionary of lists, where\n",
    "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "    (**) list or array of length nS\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA, P, isd):\n",
    "        self.P = P\n",
    "        self.isd = isd\n",
    "        self.lastaction = None  # for rendering\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "\n",
    "        self.seed()\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        return int(self.s)\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        return (int(s), r, d, {\"prob\": p})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "MAP = [\n",
    "    \"+-------------------+\",\n",
    "    \"| :A| : :B: : | :C| |\",\n",
    "    \"| : | : | : | : | : |\",\n",
    "    \"| : | : | : | : | : |\",\n",
    "    \"| : : : | : : : : : |\",\n",
    "    \"| : | : : : | : | : |\",\n",
    "    \"| : | : | : | : | : |\",\n",
    "    \"| : | : : : | : | : |\",\n",
    "    \"| | : : | : : | : : |\",\n",
    "    \"| :D| : :E: : : |F| |\",\n",
    "    \"| | : : | : | | : : |\",\n",
    "    \"+-------------------+\",\n",
    "]\n",
    "\n",
    "class TaxiEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    There are 8 designated locations in the grid world indicated by A, B, C, D, E, F . When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "    Observations:\n",
    "    There are 4200 discrete states since there are 100 taxi positions, 7 possible locations of the passenger (including the case when the passenger is in the taxi), and 6 destination locations. \n",
    "\n",
    "    Passenger locations:\n",
    "    - 0: A\n",
    "    - 1: B\n",
    "    - 2: C\n",
    "    - 3: D\n",
    "    - 4: E\n",
    "    - 5: F\n",
    "    - 6: in taxi\n",
    "    \n",
    "    Destinations:\n",
    "    - 0: A\n",
    "    - 1: B\n",
    "    - 2: C\n",
    "    - 3: D\n",
    "    - 4: E\n",
    "    - 5: F\n",
    "    \n",
    "    \n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move up\n",
    "    - 1: move down\n",
    "    - 2: move left\n",
    "    - 3: move right\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "\n",
    "    Rewards:\n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +10,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -5.\n",
    "\n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - red: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (A, B, C, D, E, F): locations for passengers and destinations\n",
    "    state space is represented by:\n",
    "        (taxi_row, taxi_col, passenger_location, destination)\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        # c for character\n",
    "        self.desc = np.asarray(MAP, dtype='c')\n",
    "\n",
    "        self.locs = locs = [(0, 1), (0, 4), (0, 8), (8, 1), (8, 4), (8, 8)]\n",
    "\n",
    "        num_states = 4200\n",
    "        num_rows = 10\n",
    "        num_columns = 10\n",
    "        max_row = num_rows - 1\n",
    "        max_col = num_columns - 1\n",
    "        initial_state_distrib = np.zeros(num_states)\n",
    "        num_actions = 6\n",
    "        P = {state: {action: [] for action in range(num_actions)} for state in range(num_states)}\n",
    "        \n",
    "        for row in range(num_rows):\n",
    "\n",
    "            for col in range(num_columns):\n",
    "\n",
    "                for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "\n",
    "                    for dest_idx in range(len(locs)):\n",
    "                        state = self.encode(row, col, pass_idx, dest_idx)\n",
    "\n",
    "                        if pass_idx < 6 and pass_idx != dest_idx:\n",
    "                            initial_state_distrib[state] += 1\n",
    "\n",
    "                        for action in range(num_actions):\n",
    "                            # defaults\n",
    "                            new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "                            reward = -1  # default reward when there is no pickup/dropoff\n",
    "                            done = False\n",
    "                            taxi_loc = (row, col)\n",
    "\n",
    "                            if action == 0:\n",
    "                                new_row = min(row + 1, max_row)\n",
    "\n",
    "                            elif action == 1:\n",
    "                                new_row = max(row - 1, 0)\n",
    "\n",
    "                            # change it bytes\n",
    "                            if action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                                new_col = min(col + 1, max_col)\n",
    "\n",
    "                            elif action == 3 and self.desc[1 + row, 2 * col] == b\":\":\n",
    "                                new_col = max(col - 1, 0)\n",
    "\n",
    "                            elif action == 4:  # pickup\n",
    "\n",
    "                                if (pass_idx < 6 and taxi_loc == locs[pass_idx]):\n",
    "                                    new_pass_idx = 6\n",
    "\n",
    "                                else:  # passenger not at location\n",
    "                                    reward = -1\n",
    "\n",
    "                            elif action == 5:  # dropoff\n",
    "\n",
    "                                if (taxi_loc == locs[dest_idx]) and pass_idx == 6:\n",
    "                                    new_pass_idx = dest_idx\n",
    "                                    done = True\n",
    "                                    reward = 10\n",
    "\n",
    "                                elif (taxi_loc in locs) and pass_idx == 6:\n",
    "                                    new_pass_idx = locs.index(taxi_loc)\n",
    "\n",
    "                                else:  # dropoff at wrong location\n",
    "                                    reward = -5\n",
    "\n",
    "                            new_state = self.encode(new_row, new_col, new_pass_idx, dest_idx)\n",
    "\n",
    "                            P[state][action].append((1.0, new_state, reward, done))\n",
    "\n",
    "        initial_state_distrib /= initial_state_distrib.sum()\n",
    "\n",
    "        discrete.DiscreteEnv.__init__(\n",
    "            self, num_states, num_actions, P, initial_state_distrib)\n",
    "\n",
    "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "        # (10) 10, 7, 6\n",
    "        i = taxi_row\n",
    "        i *= 10\n",
    "        i += taxi_col\n",
    "        i *= 7\n",
    "        i += pass_loc\n",
    "        i *= 6\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        out = []\n",
    "        out.append(i % 6)\n",
    "        i = i // 6\n",
    "        out.append(i % 7)\n",
    "        i = i // 7\n",
    "        out.append(i % 10)\n",
    "        i = i // 10\n",
    "        out.append(i)\n",
    "        assert 0 <= i < 10\n",
    "        return reversed(out)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        out = self.desc.copy().tolist()\n",
    "        # UTF-8 is a byte oriented encoding. \n",
    "        # The encoding specifies that each character is represented by a specific sequence of one or more bytes.\n",
    "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
    "        \n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
    "        \n",
    "\n",
    "        #print(\"taxi_row:{}, taxi_col: {}, pass_idx: {}, dest_idx: {}\".format(taxi_row, taxi_col, pass_idx, dest_idx))\n",
    "        \n",
    "\n",
    "        def ul(x): return \"_\" if x == \" \" else x\n",
    "\n",
    "        if pass_idx < 8:\n",
    "            #print(\"pass_idx: {}\".format(pass_idx))\n",
    "            #print(\"[1 + taxi_row]: {}, [2 * taxi_col + 1]: {}\".format([1 + taxi_row],[2 * taxi_col + 1]))\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                out[1 + taxi_row][2 * taxi_col + 1], 'red', highlight=True)\n",
    "            #print(\"out[1 + taxi_row][2 * taxi_col + 1]: {}\".format(out[1 + taxi_row][2 * taxi_col + 1]))\n",
    "            \n",
    "            pi, pj = self.locs[pass_idx]\n",
    "            #print(\"\\npi: {}, pj: {}\".format(pi, pj))\n",
    "            #print(\"[1 + pi]: {}, [2 * pj + 1]: {}\".format([1 + pi],[2 * pj + 1]))\n",
    "            out[1 + pi][2 * pj + 1] = utils.colorize(\n",
    "                out[1 + pi][2 * pj + 1], 'blue', bold=True)\n",
    "            #print(\"out[1 + pi][2 * pj + 1]: {}\".format(out[1 + pi][2 * pj + 1]))\n",
    "            \n",
    "        else:  # passenger in taxi\n",
    "            #print(\"[1 + taxi_row]: {}, [2 * taxi_col + 1]: {}\".format([1 + taxi_row],[2 * taxi_col + 1]))\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                ul(out[1 + taxi_row][2 * taxi_col + 1]), 'green', highlight=True)\n",
    "            #print(\"out[1 + taxi_row][2 * taxi_col + 1]: {}\".format(out[1 + taxi_row][2 * taxi_col + 1]))\n",
    "\n",
    "        di, dj = self.locs[dest_idx]\n",
    "        #print(\"\\ndi: {}, dj: {}\".format(di, dj))\n",
    "        #print(\"[1 + di]: {}, [2 * dj + 1]: {}\".format([1 + di],[2 * dj + 1]))\n",
    "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n",
    "        #print(\"out[1 + di][2 * dj + 1]: {}\".format(out[1 + di][2 * dj + 1]))\n",
    "        \n",
    "        \n",
    "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "        # No need to return anything for human\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='smart_cab-v2',\n",
    "    entry_point='smart_cab.envs:TaxiEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uv8pNbHmLHIH",
    "outputId": "de8ea0c5-6b4e-40fe-f2ff-e4b32e8dfda6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# core gym interface is env\n",
    "env = gym.make('smart_cab:smart_cab-v2')\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIJYYGNRLVFP",
    "outputId": "f8cee17a-7306-48f7-a771-544997afa1bc"
   },
   "outputs": [],
   "source": [
    "# env.reset(): Resets the environment and returns a random initial state.\n",
    "env.reset() \n",
    "\n",
    "# env.render(): Renders one frame of the environment (helpful in visualizing the environment)\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "text = \"\"\"\n",
    "The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "\n",
    "The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "\n",
    "A, B, C, D, E, F are the possible pickup and destination locations. \n",
    "\n",
    "The blue letter represents the current passenger pick-up location.\n",
    "\n",
    "The pink letter is the current drop-off location.\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhWbC3_FLcvg",
    "outputId": "99852e5f-5116-4d8b-86e4-f4452d850850"
   },
   "outputs": [],
   "source": [
    "# (taxi row, taxi column, passenger location index, drop-off location index)\n",
    "# Pick-up/Drop-off --> A - 0, B - 1, C - 2, D - 3, E - 4, F - 5\n",
    "# Manually set the state and  give it to the environment\n",
    "state = env.encode(0, 1, 2, 3) \n",
    "print(\"State:\", state)\n",
    "\n",
    "# A number is generated corresponding to a state between 0 and 4200, which turns out to be 57.\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdlGWj58Klbi",
    "outputId": "997cf3f0-edab-4b6c-9ef7-e8d0aef40ccf"
   },
   "outputs": [],
   "source": [
    "# Reward Table\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Output is default reward values assigned to each state.\n",
    "\n",
    "This dictionary has the structure {action: [(probability, nextstate, reward, done)]}.\n",
    "\n",
    "The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    "\n",
    "In this env, probability is always 1.0.\n",
    "\n",
    "The nextstate is the state we would be in if we take the action at this index of the dict\n",
    "\n",
    "All the movement actions have a -1 reward and the pickup/dropoff actions have -5 reward in this particular state. \n",
    "\n",
    "If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 10 at the dropoff action (5)\n",
    "\n",
    "\"\"done\"\" is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
    "\n",
    "If the taxi hits the wall, it will accumulate a -1 as well and this will affect a the long-term reward.\n",
    "\"\"\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "env.P[57]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts3Cz-h-8CBN"
   },
   "source": [
    "# **Without Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsCNyzBcFWHG",
    "outputId": "8f483238-605d-4405-f2e7-6fe4a98cbdc8"
   },
   "outputs": [],
   "source": [
    "# Without Reinforcement Learning\n",
    "\n",
    "env.s = 431  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "\n",
    "# create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 10\n",
    "while not done:\n",
    "  # take the next action\n",
    "  action = env.action_space.sample()\n",
    "  state, reward, done, info = env.step(action)\n",
    "\n",
    "  if reward == -5:\n",
    "      penalties += 1\n",
    "\n",
    "  epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n",
    "\n",
    "text = \"\"\"\n",
    "The agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
    "\n",
    "This is because we aren't learning from past experience. \n",
    "\n",
    "It can run this over and over, and it will never optimize as the agent has no memory of which action was best for each state.\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cFt_0iU8WGs"
   },
   "source": [
    "# **With Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItqJOKtE8Z-k",
    "outputId": "f628f920-a12b-41b1-94c1-e0a48d4be15f"
   },
   "outputs": [],
   "source": [
    "# train the agent\n",
    "# initialise the Q-table to a 500 x 6 matrix of zeros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# row --> state size, column --> action size\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDT_HQMaoRpx",
    "outputId": "658cd324-3bb8-48f9-aa59-df01dc7f8184"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# learning rate 0< α ≤1, the extent to which our Q-values are being updated in every iteration.\n",
    "alpha = 0.1\n",
    "\n",
    "# discount factor (0≤ γ ≤1), determines how much importance we want to give to future rewards\n",
    "# A high value for the discount factor (close to 1) captures the long-term effective award\n",
    "# A low value of a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n",
    "gamma = 0.6\n",
    "\n",
    "# prevent the action from always taking the same route, and possibly overfitting\n",
    "# punish the agent if it only exploit learned values using the Q-tables\n",
    "# Lower epsilon value results in episodes with more penalties\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "# run 10,000 times for the agent to drop the passenger at the correct location\n",
    "for i in range(1, 100001):\n",
    "  state = env.reset()\n",
    "\n",
    "  epochs, penalties, reward, = 0, 0, 0\n",
    "  done = False\n",
    "  \n",
    "  while not done:\n",
    "      # decide to pick a random action or to exploit the already computed Q-values\n",
    "      # initialise the action based on a random number with the epsilon as the benchmark\n",
    "      if random.uniform(0, 1) < epsilon:\n",
    "          action = env.action_space.sample() # Explore action space\n",
    "      else:\n",
    "          action = np.argmax(q_table[state]) # Exploit learned values\n",
    "          \n",
    "\n",
    "      # execute the choosen action in the environment to obtain the \"next_state\" and the \"reward\" from performing the action\n",
    "      next_state, reward, done, info = env.step(action) \n",
    "   \n",
    "      \n",
    "      # current state and the action to be performed\n",
    "      old_value = q_table[state, action]\n",
    "      \n",
    "      # get the highest action value (out of the 6 actions) based on the \"next_state\"\n",
    "      next_max = np.max(q_table[next_state])\n",
    "\n",
    "      \n",
    "      # calculate the updated value\n",
    "      # first, take a weight (1−α) of the old Q-value, then adding the learned value.\n",
    "      # learned value is the reward for taking the current action in the current state plus the discounted maximum reward from the next state we will be in once we take the current action\n",
    "      # learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state\n",
    "      new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "\n",
    "      # update the value on that state and after the action has been performed\n",
    "      q_table[state, action] = new_value\n",
    "\n",
    "      # penalties for dropping the passenger at the wrong location\n",
    "      if reward == -5:\n",
    "          penalties += 1\n",
    "  \n",
    "  \n",
    "\n",
    "      # go on to the next state\n",
    "      state = next_state\n",
    "      epochs += 1\n",
    "  \n",
    "\n",
    "\n",
    "      \n",
    "  if i % 1000 == 0:\n",
    "      clear_output(wait=True)\n",
    "      print(f\"Episode: {i}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Tcc-iho80i3",
    "outputId": "bf08ea63-1461-4b23-e243-27668ed65789"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "0 = down\n",
    "1 = up\n",
    "2 = right\n",
    "3 = left\n",
    "4 = pickup\n",
    "5 = dropoff\n",
    "\"\"\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "text_2 = \"\"\"\n",
    "The max Q-value is \"up\" {}, showing that Q-learning has effectively learned the best action to take in that current state!\n",
    "\"\"\".format(np.max(q_table[57]))\n",
    "\n",
    "print(\"q_table[57]: {}\".format(q_table[57]))\n",
    "print(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ks-1khOK-WcO",
    "outputId": "ab71c879-5fe8-4dcc-8530-b1be792da710"
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 10\n",
    "\n",
    "# For all 10 successful passenger drop-off\n",
    "for i in range(episodes):\n",
    "\n",
    "    # Resets the environment and returns a random initial state\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # use the current q table with its current state to do the next action\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -5:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "# 0 penalties incurred after Reinforcement Learning\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task1.1C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
