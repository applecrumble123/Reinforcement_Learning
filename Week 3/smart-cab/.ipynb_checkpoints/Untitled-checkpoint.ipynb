{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99d305a34176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m register(\n\u001b[1;32m      4\u001b[0m     \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'smart_cab-v2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     entry_point='smart_cab.envs:TaxiEnv')\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='smart_cab-v2',\n",
    "    entry_point='smart_cab.envs:TaxiEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# core gym interface is env\n",
    "env = gym.make('smart_cab:smart_cab-v2')\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset(): Resets the environment and returns a random initial state.\n",
    "env.reset() \n",
    "\n",
    "# env.render(): Renders one frame of the environment (helpful in visualizing the environment)\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "text = \"\"\"\n",
    "The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "\n",
    "The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "\n",
    "A, B, C, D, E, F are the possible pickup and destination locations. \n",
    "\n",
    "The blue letter represents the current passenger pick-up location.\n",
    "\n",
    "The pink letter is the current drop-off location.\n",
    "\"\"\"\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (taxi row, taxi column, passenger location index, drop-off location index)\n",
    "# Pick-up/Drop-off --> A - 0, B - 1, C - 2, D - 3, E - 4, F - 5\n",
    "# Manually set the state and  give it to the environment\n",
    "state = env.encode(0, 1, 2, 3) \n",
    "print(\"State:\", state)\n",
    "\n",
    "# A number is generated corresponding to a state between 0 and 4200, which turns out to be 57.\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reward Table\n",
    "\n",
    "text = \"\"\"\n",
    "Output is default reward values assigned to each state.\n",
    "\n",
    "This dictionary has the structure {action: [(probability, nextstate, reward, done)]}.\n",
    "\n",
    "The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    "\n",
    "Probability of 1.0 for taking an action to reach a state and 0.0 if the action nvr reach the state\n",
    "\n",
    "The nextstate is the state we would be in if we take the action at this index of the dict\n",
    "\n",
    "All the movement actions have a -1 reward, -3 for a wrong pickup and -10 for a wrong dropoff.\n",
    "\n",
    "If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
    "\n",
    "\"\"done\"\" is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
    "\n",
    "If the taxi hits the wall, it will accumulate a -1 as well and this will affect a the long-term reward.\n",
    "\"\"\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "env.P[57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(env.P[57]))\n",
    "\n",
    "from mdp import MDP\n",
    "\"\"\"\n",
    "for state_num in range(500):\n",
    "    print(env.P[state_num])\n",
    "    print()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "print(env.P[57])\n",
    "print()\n",
    "print(env.P[57][0])\n",
    "print()\n",
    "print(env.P[57][0][0])\n",
    "print()\n",
    "print(env.P[57][0][0][0])\n",
    "print()\n",
    "print(env.P[57][0][0][1])\n",
    "print()\n",
    "print(env.P[57][0][0][2])\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "states = []\n",
    "\n",
    "\n",
    "for state_num in range(108):\n",
    "    states.append(\"s{}\".format(state_num))\n",
    "\n",
    "state_trans_prob = []\n",
    "\n",
    "for i in range(108):\n",
    "    dict1 = {}\n",
    "    dict1['s{}'.format(i)] = env.P[i][0][0][0]\n",
    "    state_trans_prob.append(dict1)\n",
    "\n",
    "\n",
    "action_key_list = ['a0', 'a1', 'a2', 'a4', 'a5', 'a6']\n",
    "\n",
    "\n",
    "transition_probs = {}\n",
    "for state_num in range(108):\n",
    "    \n",
    "    per_state_dict = {}\n",
    "    \n",
    "    for action_key in range(6):\n",
    "        per_state_dict[action_key_list[action_key]] = state_trans_prob[env.P[state_num][action_key][0][1]]\n",
    "    transition_probs[states[state_num]] = per_state_dict\n",
    "\n",
    "#print(transition_probs)\n",
    "\n",
    "rewards = {}\n",
    "    \n",
    "for state_num in range(108):\n",
    "    \n",
    "    per_state_dict = {}\n",
    "    \n",
    "    for action_key in range(6):\n",
    "        per_action_dict = {}\n",
    "        per_action_dict['s{}'.format(env.P[state_num][action_key][0][1])] = env.P[state_num][action_key][0][2]\n",
    "        per_state_dict[action_key_list[action_key]] = per_action_dict\n",
    "    rewards[states[state_num]] = per_state_dict\n",
    "\n",
    "#print(rewards)\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mdp import has_graphviz\n",
    "from IPython.display import display\n",
    "print(\"Graphviz available:\", has_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if has_graphviz:\n",
    "    from mdp import plot_graph, plot_graph_with_state_values, plot_graph_optimal_strategy_and_state_values\n",
    "    display(plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# initialise a random policy where for each action for all states is initialised with a random value\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "total_epochs, total_rewards = 0, 0\n",
    "\n",
    "all_epochs = []\n",
    "all_reward = []\n",
    "\n",
    "total_episodes = 100\n",
    "\n",
    "for i in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    epoch_per_episode = 0\n",
    "    reward_per_episode = 0\n",
    "    done = False\n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)  \n",
    "        epoch_per_episode = epoch_per_episode + 1\n",
    "        reward_per_episode = reward_per_episode + reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    total_epochs = total_epochs + epoch_per_episode\n",
    "    total_rewards = total_rewards + reward_per_episode\n",
    "    \n",
    "    all_epochs.append(epoch_per_episode)\n",
    "    all_reward.append(reward_per_episode)\n",
    "\n",
    "mean_steps_per_episode = total_epochs/total_episodes\n",
    "mean_reward_per_episode = total_rewards/total_episodes\n",
    "\n",
    "print(\"The average steps per episode is {}\".format(mean_steps_per_episode))\n",
    "print(\"The average reward per episode is {}\".format(mean_reward_per_episode))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "\n",
    "Args:\n",
    "    policy: [S, A] shaped matrix representing the policy.\n",
    "    \n",
    "    env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "        env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "        env.nS is a number of states in the environment. \n",
    "        env.nA is a number of actions in the environment.\n",
    "    \n",
    "    theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "    \n",
    "    discount_factor: Gamma discount factor.\n",
    "\n",
    "Returns:\n",
    "    Vector of length env.nS representing the value function.\n",
    "    \n",
    "\"\"\"\n",
    "# Initialise the state-value with 0\n",
    "V = np.zeros(env.nS)\n",
    "# initialise a random policy for which the value of each action in all state is the probability of taking an action\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "# policy evaluation\n",
    "def policy_eval(policy, discount_factor=1.0, theta=0.00001):\n",
    "    \n",
    "    for i in range(1000):\n",
    "\n",
    "        #delta = change in value of state from one iteration to next\n",
    "        # there is no change so initialise with 0\n",
    "        delta = 0  \n",
    "\n",
    "        #for all states\n",
    "        for state in range(env.nS):  \n",
    "            #print(state)\n",
    "\n",
    "            #initiate value of the state as 0\n",
    "            val = 0  \n",
    "\n",
    "             #for all actions/action probabilities\n",
    "            for action, action_probability in enumerate(random_policy[state]):\n",
    "                #print(action, act_prob)\n",
    "\n",
    "                #transition probabilities,state,rewards of each action\n",
    "                for trans_prob, next_state, reward, done in env.P[state][action]:\n",
    "                    #print(prob, next_state, reward, done)\n",
    "\n",
    "                    # equation to calculate the value of the state\n",
    "                    # action_probability = probability of taking action a in state s under policy π\n",
    "                    val = val + (action_probability * trans_prob) * (reward + discount_factor * V[next_state])\n",
    "\n",
    "                    # the change would be the max value between the initial change and the current change in value\n",
    "                    delta = max(delta, np.abs(val-V[state]))\n",
    "\n",
    "                    # the current state would have that value\n",
    "                    V[state] = val\n",
    "\n",
    "        #break if the change in value is less than the threshold (theta)\n",
    "        if delta < theta: \n",
    "                break\n",
    "    \n",
    "    return np.array(V)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "\n",
    "\n",
    "def action_value(state, A):\n",
    "    discount_factor = 0.95\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to calculate the value for all action in a given state.\n",
    "\n",
    "    Args:\n",
    "        state: The state to consider (int)\n",
    "        V: The value to use as an estimator, Vector of length env.nS\n",
    "\n",
    "    Returns:\n",
    "        A vector of length env.nA containing the expected value of each action.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, done in env.P[state][a]:\n",
    "            A[a] =  A[a] + prob * (reward + discount_factor * V[next_state])\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " Policy Improvement Algorithm. Iteratively evaluates and improves a policy until an optimal policy is found.\n",
    "    \n",
    "   \n",
    "    env: The OpenAI envrionment.\n",
    "\n",
    "    policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "        policy, env, discount_factor.\n",
    "\n",
    "    discount_factor: gamma discount factor.\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        \n",
    "        Policy is the optimal policy, a matrix of shape [S, A] where each state s contains a valid \n",
    "        probability distribution over actions.\n",
    "        \n",
    "        V is the value function for the optimal policy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # evaluate current policy\n",
    "    curr_pol_val = policy_eval(policy = random_policy, discount_factor=0.95, theta=0.00001)\n",
    "    \n",
    "    # Check if policy did improve (Set it as True first)\n",
    "    policy_stable = True  \n",
    "    \n",
    "    # for each states\n",
    "    for state in range(env.nS):\n",
    "        \n",
    "        # best action (Highest prob) under current policy\n",
    "        chosen_act = np.argmax(random_policy[state])\n",
    "        \n",
    "        # find action values in that given state\n",
    "        act_values = action_value(state, curr_pol_val) \n",
    "        \n",
    "        # policy improvement\n",
    "        #find best action\n",
    "        best_act = np.argmax(act_values) \n",
    "        \n",
    "        if chosen_act != best_act:\n",
    "            #Greedily find best action\n",
    "            policy_stable = False  \n",
    "        \n",
    "        #update     \n",
    "        random_policy[state] = np.eye(env.nA)[best_act]  \n",
    "    \n",
    "    if policy_stable:\n",
    "        \n",
    "        print(random_policy)\n",
    "        \n",
    "        print(curr_pol_val)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_policy(policy):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    total_reward = 0\n",
    "    for i in range(1000):\n",
    "        state, reward, done, info = env.step(np.argmax(policy[curr_state])) \n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "        total_reward = total_reward + reward\n",
    "        env.s = curr_state\n",
    "        #env.render()\n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_policy(random_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
