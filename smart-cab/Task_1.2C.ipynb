{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "def categorical_sample(prob_n, np_random):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "\n",
    "class DiscreteEnv(Env):\n",
    "\n",
    "    \"\"\"\n",
    "    Has the following members\n",
    "    - nS: number of states\n",
    "    - nA: number of actions\n",
    "    - P: transitions (*)\n",
    "    - isd: initial state distribution (**)\n",
    "    (*) dictionary of lists, where\n",
    "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "    (**) list or array of length nS\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA, P, isd):\n",
    "        self.P = P\n",
    "        self.isd = isd\n",
    "        self.lastaction = None  # for rendering\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "\n",
    "        self.seed()\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        return int(self.s)\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        return (int(s), r, d, {\"prob\": p})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "MAP = [\n",
    "    \"+-------------------+\",\n",
    "    \"| :A| : :B: : | :C| |\",\n",
    "    \"| : | : | : | : | : |\",\n",
    "    \"| : | : | : | : | : |\",\n",
    "    \"| : : : | : : : : : |\",\n",
    "    \"| : | : : : | : | : |\",\n",
    "    \"| : | : | : | : | : |\",\n",
    "    \"| : | : : : | : | : |\",\n",
    "    \"| | : : | : : | : : |\",\n",
    "    \"| :D| : :E: : : |F| |\",\n",
    "    \"| | : : | : | | : : |\",\n",
    "    \"+-------------------+\",\n",
    "]\n",
    "\n",
    "class TaxiEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    There are 8 designated locations in the grid world indicated by A, B, C, D, E, F . When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "    Observations:\n",
    "    There are 4200 discrete states since there are 100 taxi positions, 7 possible locations of the passenger (including the case when the passenger is in the taxi), and 6 destination locations. \n",
    "\n",
    "    Passenger locations:\n",
    "    - 0: A\n",
    "    - 1: B\n",
    "    - 2: C\n",
    "    - 3: D\n",
    "    - 4: E\n",
    "    - 5: F\n",
    "    - 6: in taxi\n",
    "    \n",
    "    Destinations:\n",
    "    - 0: A\n",
    "    - 1: B\n",
    "    - 2: C\n",
    "    - 3: D\n",
    "    - 4: E\n",
    "    - 5: F\n",
    "    \n",
    "    \n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move up\n",
    "    - 1: move down\n",
    "    - 2: move left\n",
    "    - 3: move right\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "\n",
    "    Rewards:\n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +10,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -5.\n",
    "\n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - red: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (A, B, C, D, E, F, G, H): locations for passengers and destinations\n",
    "    state space is represented by:\n",
    "        (taxi_row, taxi_col, passenger_location, destination)\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.desc = np.asarray(MAP, dtype='c')\n",
    "\n",
    "        self.locs = locs = [(0, 1), (0, 4), (0, 8), (8, 1), (8, 4), (8, 8)]\n",
    "\n",
    "        num_states = 4200\n",
    "        num_rows = 10\n",
    "        num_columns = 10\n",
    "        max_row = num_rows - 1\n",
    "        max_col = num_columns - 1\n",
    "        initial_state_distrib = np.zeros(num_states)\n",
    "        num_actions = 6\n",
    "        P = {state: {action: []\n",
    "                     for action in range(num_actions)} for state in range(num_states)}\n",
    "        for row in range(num_rows):\n",
    "\n",
    "            for col in range(num_columns):\n",
    "\n",
    "                for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "\n",
    "                    for dest_idx in range(len(locs)):\n",
    "                        state = self.encode(row, col, pass_idx, dest_idx)\n",
    "\n",
    "                        if pass_idx < 6 and pass_idx != dest_idx:\n",
    "                            initial_state_distrib[state] += 1\n",
    "\n",
    "                        for action in range(num_actions):\n",
    "                            # defaults\n",
    "                            new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "                            reward = -1  # default reward when there is no pickup/dropoff\n",
    "                            done = False\n",
    "                            taxi_loc = (row, col)\n",
    "\n",
    "                            if action == 0:\n",
    "                                new_row = min(row + 1, max_row)\n",
    "\n",
    "                            elif action == 1:\n",
    "                                new_row = max(row - 1, 0)\n",
    "\n",
    "                            if action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                                new_col = min(col + 1, max_col)\n",
    "\n",
    "                            elif action == 3 and self.desc[1 + row, 2 * col] == b\":\":\n",
    "                                new_col = max(col - 1, 0)\n",
    "\n",
    "                            elif action == 4:  # pickup\n",
    "\n",
    "                                if (pass_idx < 6 and taxi_loc == locs[pass_idx]):\n",
    "                                    new_pass_idx = 6\n",
    "\n",
    "                                else:  # passenger not at location\n",
    "                                    reward = -3\n",
    "\n",
    "                            elif action == 5:  # dropoff\n",
    "\n",
    "                                # if the taxi decides to drop off and it reaches the correct destination after exploring all the destination, reward is 10 and the experiment stop\n",
    "                                if (taxi_loc == locs[dest_idx] and pass_idx == 6):\n",
    "                                    new_pass_idx = dest_idx\n",
    "                                    done = True\n",
    "                                    reward = 10\n",
    "\n",
    "                                # if the taxi decides to drop off and hasnt reach the correct destination, reward is -5\n",
    "                                elif (taxi_loc in locs):\n",
    "                                    new_pass_idx = locs.index(taxi_loc)\n",
    "\n",
    "                                    if (new_pass_idx != dest_idx):\n",
    "                                        done = False\n",
    "                                        reward = -5\n",
    "\n",
    "                              \n",
    "\n",
    "                            new_state = self.encode(new_row, new_col, new_pass_idx, dest_idx)\n",
    "\n",
    "                            P[state][action].append((1.0, new_state, reward, done))\n",
    "\n",
    "        initial_state_distrib /= initial_state_distrib.sum()\n",
    "\n",
    "        discrete.DiscreteEnv.__init__(\n",
    "            self, num_states, num_actions, P, initial_state_distrib)\n",
    "\n",
    "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "        # (10) 10, 7, 6\n",
    "        i = taxi_row\n",
    "        i *= 10\n",
    "        i += taxi_col\n",
    "        i *= 7\n",
    "        i += pass_loc\n",
    "        i *= 6\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        out = []\n",
    "        out.append(i % 6)\n",
    "        i = i // 6\n",
    "        out.append(i % 7)\n",
    "        i = i // 7\n",
    "        out.append(i % 10)\n",
    "        i = i // 10\n",
    "        out.append(i)\n",
    "        assert 0 <= i < 10\n",
    "        return reversed(out)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        out = self.desc.copy().tolist()\n",
    "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
    "        \n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
    "        \n",
    "\n",
    "        #print(\"taxi_row:{}, taxi_col: {}, pass_idx: {}, dest_idx: {}\".format(taxi_row, taxi_col, pass_idx, dest_idx))\n",
    "        \n",
    "\n",
    "        def ul(x): return \"_\" if x == \" \" else x\n",
    "\n",
    "        if pass_idx < 8:\n",
    "            #print(\"pass_idx: {}\".format(pass_idx))\n",
    "            #print(\"[1 + taxi_row]: {}, [2 * taxi_col + 1]: {}\".format([1 + taxi_row],[2 * taxi_col + 1]))\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                out[1 + taxi_row][2 * taxi_col + 1], 'red', highlight=True)\n",
    "            #print(\"out[1 + taxi_row][2 * taxi_col + 1]: {}\".format(out[1 + taxi_row][2 * taxi_col + 1]))\n",
    "            \n",
    "            pi, pj = self.locs[pass_idx]\n",
    "            #print(\"\\npi: {}, pj: {}\".format(pi, pj))\n",
    "            #print(\"[1 + pi]: {}, [2 * pj + 1]: {}\".format([1 + pi],[2 * pj + 1]))\n",
    "            out[1 + pi][2 * pj + 1] = utils.colorize(\n",
    "                out[1 + pi][2 * pj + 1], 'blue', bold=True)\n",
    "            #print(\"out[1 + pi][2 * pj + 1]: {}\".format(out[1 + pi][2 * pj + 1]))\n",
    "            \n",
    "        else:  # passenger in taxi\n",
    "            #print(\"[1 + taxi_row]: {}, [2 * taxi_col + 1]: {}\".format([1 + taxi_row],[2 * taxi_col + 1]))\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                ul(out[1 + taxi_row][2 * taxi_col + 1]), 'green', highlight=True)\n",
    "            #print(\"out[1 + taxi_row][2 * taxi_col + 1]: {}\".format(out[1 + taxi_row][2 * taxi_col + 1]))\n",
    "\n",
    "        di, dj = self.locs[dest_idx]\n",
    "        #print(\"\\ndi: {}, dj: {}\".format(di, dj))\n",
    "        #print(\"[1 + di]: {}, [2 * dj + 1]: {}\".format([1 + di],[2 * dj + 1]))\n",
    "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n",
    "        #print(\"out[1 + di][2 * dj + 1]: {}\".format(out[1 + di][2 * dj + 1]))\n",
    "        \n",
    "        \n",
    "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "        # No need to return anything for human\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='smart_cab-v2',\n",
    "    entry_point='smart_cab.envs:TaxiEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# core gym interface is env\n",
    "env = gym.make('smart_cab:smart_cab-v2')\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset(): Resets the environment and returns a random initial state.\n",
    "env.reset() \n",
    "\n",
    "# env.render(): Renders one frame of the environment (helpful in visualizing the environment)\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "text = \"\"\"\n",
    "The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "\n",
    "The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "\n",
    "A, B, C, D, E, F are the possible pickup and destination locations. \n",
    "\n",
    "The blue letter represents the current passenger pick-up location.\n",
    "\n",
    "The pink letter is the current drop-off location.\n",
    "\"\"\"\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (taxi row, taxi column, passenger location index, drop-off location index)\n",
    "# Pick-up/Drop-off --> A - 0, B - 1, C - 2, D - 3, E - 4, F - 5\n",
    "# Manually set the state and  give it to the environment\n",
    "state = env.encode(0, 1, 2, 3) \n",
    "print(\"State:\", state)\n",
    "\n",
    "# A number is generated corresponding to a state between 0 and 4200, which turns out to be 57.\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Table\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Output is default reward values assigned to each state.\n",
    "\n",
    "This dictionary has the structure {action: [(probability, nextstate, reward, done)]}.\n",
    "\n",
    "The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    "\n",
    "In this env, probability is always 1.0.\n",
    "\n",
    "The nextstate is the state we would be in if we take the action at this index of the dict\n",
    "\n",
    "All the movement actions have a -1 reward and the pickup/dropoff actions have -5 reward in this particular state. \n",
    "\n",
    "If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 10 at the dropoff action (5)\n",
    "\n",
    "\"\"done\"\" is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
    "\n",
    "If the taxi hits the wall, it will accumulate a -1 as well and this will affect a the long-term reward.\n",
    "\"\"\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "env.P[57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Without Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s = 57  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "\n",
    "# create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 10\n",
    "while not done:\n",
    "  # take the next action\n",
    "  action = env.action_space.sample()\n",
    "  state, reward, done, info = env.step(action)\n",
    "\n",
    "  if reward == -5 or reward == -3:\n",
    "      penalties += 1\n",
    "\n",
    "  epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n",
    "\n",
    "text = \"\"\"\n",
    "The agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
    "\n",
    "This is because we aren't learning from past experience. \n",
    "\n",
    "It can run this over and over, and it will never optimize as the agent has no memory of which action was best for each state.\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **With Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent\n",
    "\n",
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "print(q_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 200001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -5 or reward == -3:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "0 = down\n",
    "1 = up\n",
    "2 = right\n",
    "3 = left\n",
    "4 = pickup\n",
    "5 = dropoff\n",
    "\"\"\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "action = ['down', 'up', 'right', 'left', 'pick-up', 'drop-off']\n",
    "\n",
    "index_action = list(q_table[57]).index(np.max(q_table[57]))\n",
    "\n",
    "text_2 = \"\"\"\n",
    "The max Q-value is {}, which is '{}', showing that Q-learning has effectively learned \n",
    "the best action to take in that current state!\"\"\".format(np.max(q_table[57]), action[index_action])\n",
    "\n",
    "print(\"q_table[57]: {}\".format(q_table[57]))\n",
    "print(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 10\n",
    "\n",
    "# For all 10 successful passenger drop-off\n",
    "for i in range(episodes):\n",
    "\n",
    "    # Resets the environment and returns a random initial state\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # use the current q table with its current state to do the next action\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -5 or reward == -3:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        \n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "# 0 penalties incurred after Reinforcement Learning\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
